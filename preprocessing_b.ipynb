{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bz2\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and helper function to extract relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regular expressions once.\n",
    "infobox_pattern = re.compile(r'\\{\\{Infobox [^}]+\\}\\}', flags=re.DOTALL)\n",
    "sidebar_pattern = re.compile(r'\\{\\{Sidebar [^}]+\\}\\}', flags=re.DOTALL)\n",
    "link_pattern = re.compile(r'\\[\\[([^|\\]]+\\|)?([^\\]]+)\\]\\]')\n",
    "references_pattern = re.compile(r'==\\s*(References|External links|See also|Notes)\\s*==.*', flags=re.DOTALL)\n",
    "citation_needed_pattern = re.compile(r'\\{\\{citation needed[^}]*\\}\\}', flags=re.DOTALL)\n",
    "cn_pattern = re.compile(r'\\{\\{cn\\}\\}', flags=re.DOTALL)\n",
    "curly_braces_pattern = re.compile(r'\\{\\{[^}]+\\}\\}', flags=re.DOTALL)\n",
    "whitespace_pattern = re.compile(r'\\s+')\n",
    "table_pattern = re.compile(r'\\{\\|.*?\\|\\}', flags=re.DOTALL)\n",
    "line_table_pattern = re.compile(r'\\{\\|.*$', flags=re.MULTILINE)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean Wikipedia page text by removing non-essential sections and formatting.\"\"\"\n",
    "    text = infobox_pattern.sub('', text)\n",
    "    text = sidebar_pattern.sub('', text)\n",
    "    text = table_pattern.sub('', text)\n",
    "    text = line_table_pattern.sub('', text)\n",
    "    text = link_pattern.sub(r'\\2', text)\n",
    "    text = references_pattern.sub('', text)\n",
    "    text = citation_needed_pattern.sub('', text)\n",
    "    text = cn_pattern.sub('', text)\n",
    "    text = curly_braces_pattern.sub('', text)\n",
    "    text = whitespace_pattern.sub(' ', text)\n",
    "    text = re.sub(r\"\\b\\w+\\s*=\\s*[^|{}]+\\s*\\|\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\[.*?\\]|\\(.*?\\)|http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\|\\s*\\w+\\s*=\\s*[^|{}]+\", \"\", text)\n",
    "    text = re.sub(r\"<ref[^>]*>.*?</ref>\", \"\", text)\n",
    "    text = re.sub(r\"<ref[^>]*\\/>\", \"\", text)\n",
    "    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Remove namespace from XML tag.\n",
    "def strip_namespace(tag: str) -> str:\n",
    "    return tag.split(\"}\")[-1] if \"}\" in tag else tag\n",
    "\n",
    "# Extract all category names from the wikitext.\n",
    "# Danish Wikipedia category tags look like: [[Kategori:SomeCategory]]\n",
    "def extract_categories(text: str) -> list:\n",
    "    return re.findall(r'\\[\\[Kategori:(.*?)\\]\\]', text, flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and categorizing pages - i.e. whether they are history, grammar, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 document(s) for category 'History'.\n",
      "Loaded 236 document(s) for category 'Grammar'.\n",
      "Loaded 2000 document(s) for category 'CultureTradition'.\n",
      "Loaded 2000 document(s) for category 'ArtEntertainment'.\n",
      "Loaded 2000 document(s) for category 'Geography'.\n",
      "Saved 2000 documents for category 'History' to Documents\\doc_history.json\n",
      "Saved 236 documents for category 'Grammar' to Documents\\doc_grammar.json\n",
      "Saved 2000 documents for category 'CultureTradition' to Documents\\doc_culturetradition.json\n",
      "Saved 2000 documents for category 'ArtEntertainment' to Documents\\doc_artentertainment.json\n",
      "Saved 2000 documents for category 'Geography' to Documents\\doc_geography.json\n"
     ]
    }
   ],
   "source": [
    "# Load pages from a Wikipedia bz2 dump, filter them using provided category keywords,\n",
    "# and create LangChain documents grouped by category.\n",
    "\n",
    "def load_pages_by_category(file_path: str, category_keywords: dict, max_docs_per_category: int = 1):\n",
    "    docs_by_category = {cat: [] for cat in category_keywords}\n",
    "    \n",
    "    with bz2.open(file_path, 'rb') as file:\n",
    "        for event, elem in ET.iterparse(file, events=(\"end\",)):\n",
    "            if strip_namespace(elem.tag) == \"page\":\n",
    "                title = \"No Title\"\n",
    "                raw_text = \"\"\n",
    "                # Extract title and text from the page.\n",
    "                for child in elem:\n",
    "                    tag = strip_namespace(child.tag)\n",
    "                    if tag == \"title\":\n",
    "                        title = child.text or title\n",
    "                    elif tag == \"revision\":\n",
    "                        for subchild in child:\n",
    "                            if strip_namespace(subchild.tag) == \"text\":\n",
    "                                raw_text = subchild.text or \"\"\n",
    "                                break\n",
    "                # First, do a quick check for category tags in raw text.\n",
    "                page_categories = extract_categories(raw_text)\n",
    "                # If the page contains any of the keywords for a category, process it.\n",
    "                for friendly_cat, keywords in category_keywords.items():\n",
    "                    if len(docs_by_category[friendly_cat]) < max_docs_per_category:\n",
    "                        if any(kw.lower() in cat.lower() for cat in page_categories for kw in keywords):\n",
    "                            # Only clean the text if the page matches the category.\n",
    "                            cleaned_text = clean_text(raw_text)\n",
    "                            doc = LangchainDocument(page_content=cleaned_text, \n",
    "                                                    metadata={\"title\": title, \"categories\": [friendly_cat]})\n",
    "                            docs_by_category[friendly_cat].append(doc)\n",
    "                # Clear the element to free memory.\n",
    "                elem.clear()\n",
    "                \n",
    "                # Check if we have reached the desired count for all categories.\n",
    "                if all(len(docs_by_category[cat]) >= max_docs_per_category for cat in docs_by_category):\n",
    "                    break\n",
    "    return docs_by_category\n",
    "\n",
    "# Save the LangChain documents (grouped by category) as separate JSON files.\n",
    "def save_documents_by_category(docs_by_category: dict, output_dir: str = \"Documents\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for cat, docs in docs_by_category.items():\n",
    "        documents = [{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
    "        filename = os.path.join(output_dir, f\"doc_{cat.lower()}.json\")\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(documents, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(documents)} documents for category '{cat}' to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"../data/dawiki-latest-pages-articles.xml.bz2\"\n",
    "    category_keywords = {\n",
    "        \"History\": [\"histor\", \"middelalder\", \"grundlov\"],\n",
    "        \"Grammar\": [\"grammat\", \"syntaks\", \"ordbog\"],\n",
    "        \"CultureTradition\": [\"Kult\", \"tradi\", \"reli\", \"ritual\"],\n",
    "        \"ArtEntertainment\": [\"Kunst\", \"underhold\", \"film\", \"musik\", \"teater\", \"mode\", \"TV og Radio\"],\n",
    "        \"Geography\": [\"geografi\", \"land\", \"kommune\", \"hovedstad\", \"danmark\", \"amt\", \"jorden\", \"Demografi\", \"Ekspeditioner\", \"Geod√¶si\", \"Geografer\", \"Kartografi\", \"Landsdele\"]\n",
    "    }\n",
    "    # Here we can specify how many documents we want per subject by changing \"max_docs_per_category\"\n",
    "    docs_by_category = load_pages_by_category(file_path, category_keywords, max_docs_per_category=2000)\n",
    "    \n",
    "    # Display the count per category.\n",
    "    for cat, docs in docs_by_category.items():\n",
    "        print(f\"Loaded {len(docs)} document(s) for category '{cat}'.\")\n",
    "    \n",
    "    save_documents_by_category(docs_by_category, output_dir=\"Documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
