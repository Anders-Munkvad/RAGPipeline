{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)  # This will be helpful when visualizing retriever outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import import_ipynb\n",
    "import importlib\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class.\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "# Same as the HuggingFace approach.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  \n",
    "    chunk_overlap=100,  \n",
    "    add_start_index=True,  \n",
    "    strip_whitespace=True, \n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "filename = \"Documents/combined.json\"\n",
    "\n",
    "# Load the JSON data.\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    docs_data = json.load(f)\n",
    "\n",
    "# Reconstruct the LangChain Document objects.\n",
    "RAW_KNOWLEDGE_BASE = [LangchainDocument(page_content=d[\"page_content\"], metadata=d[\"metadata\"]) for d in docs_data]\n",
    "\n",
    "# Process the documents using the text splitter.\n",
    "docs_processed = []\n",
    "for doc in RAW_KNOWLEDGE_BASE:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"Processed {len(docs_processed)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
    "\n",
    "# GTE embedding model:\n",
    "#print(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")\n",
    "\n",
    "# multi e5 embedding model:\n",
    "print(f\"Model's maximum sequence length: {SentenceTransformer('intfloat/multilingual-e5-base').max_seq_length}\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# GTE embedding model:\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
    "\n",
    "# multi e5 embedding model:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\")\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
    "\n",
    "# Plot the distribution of document lengths, counted as the number of tokens\n",
    "fig = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "# GTE embedding model:\n",
    "#EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "# e5 embedding model:\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "\n",
    "    # Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    512,  # We choose a chunk size adapted to our model (can vary from model to model, but does not in this case)\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
    "fig = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# Creating the vector database:\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    #model_kwargs={\"device\": \"cuda\"}, \n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "# Save the vector database to a folder (e.g., \"faiss_index\")\n",
    "# \"faiss_index\" is the GTE model, \"faiss_index_e5\" is the index created with the e5 embedding model\n",
    "KNOWLEDGE_VECTOR_DATABASE.save_local(\"faiss_index_e5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    #model_kwargs={\"device\": \"cuda\"}, # Can we use cuda??\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "\n",
    "vector_db_e5 = FAISS.load_local(\"Documents/faiss_index_e5\", embedding_model, allow_dangerous_deserialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a user query in the same space\n",
    "user_query = \"Hvem er Harald Bl√•tand?\"\n",
    "query_vector = embedding_model.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# The following is just visualisation:\n",
    "\n",
    "# PaCMAP\n",
    "embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "\n",
    "embeddings_2d = [\n",
    "    list(vector_db_e5.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed))\n",
    "] + [query_vector]\n",
    "\n",
    "# Fit the data (the index of transformed data corresponds to the index of the original data)\n",
    "documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.DataFrame.from_dict(\n",
    "    [\n",
    "        {\n",
    "            \"x\": documents_projected[i, 0],\n",
    "            \"y\": documents_projected[i, 1],\n",
    "            \"category\": docs_processed[i].metadata[\"categories\"][0],\n",
    "            \"title\": docs_processed[i].metadata[\"title\"],\n",
    "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
    "            \"symbol\": \"circle\",\n",
    "            \"size_col\": 4,\n",
    "        }\n",
    "        for i in range(len(docs_processed))\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            \"x\": documents_projected[-1, 0],\n",
    "            \"y\": documents_projected[-1, 1],\n",
    "            \"category\": \"Query\",\n",
    "            \"title\": \"User query\",\n",
    "            \"extract\": user_query,\n",
    "            \"symbol\": \"star\",\n",
    "            \"size_col\": 100,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"category\",              \n",
    "    hover_data=[\"title\", \"extract\"],\n",
    "    size=\"size_col\",\n",
    "    symbol=\"symbol\",\n",
    "    color_discrete_map={\"Query\": \"black\"},\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\"))\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"<b>Category</b>\",\n",
    "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example retrieval output:\n",
    "\n",
    "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
    "retrieved_docs = vector_db_e5.similarity_search(query=user_query, k=5)\n",
    "print(\"\\n==================================Top document==================================\")\n",
    "for i in range(5):\n",
    "    print(retrieved_docs[i].page_content)\n",
    "print(\"==================================Metadata======================================\")\n",
    "for i in range(5):\n",
    "    print(retrieved_docs[i].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
